//
// Micro benchmark support
//

apply plugin: 'me.champeau.gradle.jmh'

buildscript {
    ext.bench = 'PCAImputeMissingTrainingBench'
    ext.trial = 1
    ext.suffix = 'original'
}

ext {
    H2OBuildVersion bv = new H2OBuildVersion(rootDir, version)
    String date = new Date().format("yyyyMMddHHmmss", TimeZone.getTimeZone("UTC"))

    // Environment for the run
    jmhRunDate = date
    jmhRunSha = bv.getLastCommitHash()
    jmhResultFileName = "${bv.getLastCommitHash()}-${date}.csv"
    jmhResultFile = project.file("${project.buildDir}/reports/jmh/$jmhResultFileName")

    // ubench - post-processed output
    ubenchResultFile = project.file("${project.buildDir}/reports/ubench/$jmhResultFileName")
}

// Setup JMH
jmh {
    //
    // Include only selected tests for benchmarking
    //if (project.hasProperty('ubenchIncludeOnly')) {
    //    include = project.ubenchIncludeOnly
    //}

    include = ".*${bench}.*"
    // exclude = ['some regular expression'] // exclude pattern (regular expression) for benchmarks to be executed
    iterations = 30 // Number of measurement iterations to do.
    //benchmarkMode = ['thrpt','ss'] // Benchmark mode. Available modes are: [Throughput/thrpt, AverageTime/avgt, SampleTime/sample, SingleShotTime/ss, All/all]
    //batchSize = 1 // Batch size: number of benchmark method calls per operation. (some benchmark modes can ignore this setting)
    fork = 1 // How many times to forks a single benchmark. Use 0 to disable forking altogether
    //failOnError = false // Should JMH fail immediately if any benchmark had experienced the unrecoverable error?
    //forceGC = false // Should JMH force GC between iterations?
    //jvm = 'myjvm' // Custom JVM to use when forking.
    //jvmArgs = ['Custom JVM args to use when forking.']
    //jvmArgsAppend = ['Custom JVM args to use when forking (append these)']
    //jvmArgsPrepend =[ 'Custom JVM args to use when forking (prepend these)']
    humanOutputFile = project.file("${project.buildDir}/reports/jmh/human-${bench}_${suffix}_trial${trial}.txt") // human-readable output file
    resultsFile = project.file("${project.buildDir}/reports/jmh/results-${bench}.json") // results file
    //operationsPerInvocation = 10 // Operations per invocation.
    //benchmarkParameters =  [:] // Benchmark parameters.
    //profilers = [] // Use profilers to collect additional data. Supported profilers: [cl, comp, gc, stack, perf, perfnorm, perfasm, xperf, xperfasm, hs_cl, hs_comp, hs_gc, hs_rt, hs_thr]
    timeOnIteration = '1s' // Time to spend at each measurement iteration.
    resultFormat = 'JSON' // Result format type (one of CSV, JSON, NONE, SCSV, TEXT)
    //synchronizeIterations = false // Synchronize iterations?
    //threads = 4 // Number of worker threads to run with.
    //threadGroups = [2,3,4] //Override thread group distribution for asymmetric benchmarks.
    //timeout = '1s' // Timeout for benchmark iteration.
    timeUnit = 'ns' // Output time unit. Available time units are: [m, s, ms, us, ns].
    //verbosity = 'NORMAL' // Verbosity mode. Available modes are: [SILENT, NORMAL, EXTRA]
    warmup = '1s' // Time to spend at each warmup iteration.
    //warmupBatchSize = 10 // Warmup batch size: number of benchmark method calls per operation.
    warmupForks = 0 // How many warmup forks to make for a single benchmark. 0 to disable warmup forks.
    warmupIterations = 30 // Number of warmup iterations to do.
    //warmupMode = 'INDI' // Warmup mode for warming up selected benchmarks. Warmup modes are: [INDI, BULK, BULK_INDI].
   warmupBenchmarks = ['.*Warmup'] // Warmup benchmarks to include in the run in addition to already selected. JMH will not measure these benchmarks, but only use them for the warmup.

    jmhVersion = '1.21'
    resultsFile = project.jmhResultFile
    duplicateClassesStrategy = 'warn'
    resultFormat = 'JSON'
    //
    // Attach different profilers (gc, stack summary, ...)
    // See: http://java-performance.info/introduction-jmh-profilers/
    //profilers = ['stack'] // ['hs_comp']
}

dependencies {
    jmh group: 'commons-io' , name: 'commons-io', version: '2.4'
}

task jmhPostProcess(dependsOn: "jmh") {
    doFirst {
        project.ubenchResultFile.getParentFile().mkdirs()
    }
    doLast {
        def results = project.jmhResultFile.readLines()
        def header = "\"SHA\",\"Date\"," + results.head()
        def data = results.tail().collect { "\"${project.jmhRunSha}\",\"${project.jmhRunDate}\",$it" }
        project.ubenchResultFile.withWriter{ out ->
            out.println header
            data.each { out.println it }
        }
        logger.error("Ubench reported in ${project.ubenchResultFile}")
    }
}

task uploadResultsToS3(type: water.build.tasks.S3UploadTask) {
    bucket = "ai.h2o.ubench"
    objectName = "${project.name}/$jmhResultFileName"
    file = ubenchResultFile
    enabled = project.hasProperty("doUploadUBenchResults") && project.doUploadUBenchResults == "true"
}

task ubench
ubench.dependsOn jmhPostProcess
ubench.dependsOn uploadResultsToS3
uploadResultsToS3.shouldRunAfter("jmhPostProcess")
